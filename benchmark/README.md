# Neuron Model Benchmark Suite

ì„¤ì • íŒŒì¼ ê¸°ë°˜ì˜ ìœ ì—°í•œ ëª¨ë¸ ì»´íŒŒì¼ ë° ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ì…ë‹ˆë‹¤.

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
benchmark/
â”œâ”€â”€ configs/                    # ëª¨ë¸ë³„ ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ llama31-70b.conf       # Llama 3.1 70B ì„¤ì •
â”‚   â””â”€â”€ qwen3-8b.conf          # Qwen3 8B ì„¤ì •
â”œâ”€â”€ scripts/                    # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ compile_model.sh       # ëª¨ë¸ ì»´íŒŒì¼
â”‚   â”œâ”€â”€ run_benchmark.sh       # Performance ë²¤ì¹˜ë§ˆí¬ (vLLM Bench)
â”‚   â”œâ”€â”€ run_llmperf.sh         # Performance ë²¤ì¹˜ë§ˆí¬ (LLMPerf)
â”‚   â””â”€â”€ run_accuracy.sh        # Accuracy í…ŒìŠ¤íŠ¸
â”œâ”€â”€ reports/                    # ë¦¬í¬íŠ¸ ìƒì„± ë„êµ¬
â”‚   â”œâ”€â”€ generate_report.py     # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
â”‚   â””â”€â”€ generate_html_report.py # HTML ë¦¬í¬íŠ¸
â””â”€â”€ README.md                   # ì´ íŒŒì¼
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 0. ì‚¬ì „ ì¤€ë¹„

**ìë™ ì„¤ì • (ì¶”ì²œ):**
```bash
cd benchmark
chmod +x setup.sh
./setup.sh
# â†’ ì•ˆì „í•œ ì˜ì¡´ì„±ë§Œ ì„¤ì¹˜í•˜ëŠ” ì˜µì…˜ ì„ íƒ (Option 1)
```

**ìˆ˜ë™ ì„¤ì • (ì•ˆì „):**
```bash
# AWS Neuron Samples ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 
cd ~
git clone --depth 1 https://github.com/aws-neuron/aws-neuron-samples.git

# ì•ˆì „í•œ ì˜ì¡´ì„±ë§Œ ì„¤ì¹˜ (ê¸°ì¡´ vLLM í™˜ê²½ ë³´í˜¸)
pip install lm-eval datasets tiktoken openai psutil botocore

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ
chmod +x benchmark/scripts/*.sh
```

**âš ï¸ ì¤‘ìš”: ì˜ì¡´ì„± ì¶©ëŒ ë°©ì§€**

aws-neuron-samplesì˜ requirements.txtì—ëŠ” ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- `torch`, `transformers`, `pydantic>2.10`, `pyarrow==20.0.0`

ì´ë“¤ì€ ê¸°ì¡´ vLLM í™˜ê²½ê³¼ ì¶©ëŒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, **ì•ˆì „í•œ ì˜ì¡´ì„±ë§Œ ì„¤ì¹˜**í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤:
```bash
# ì•ˆì „í•œ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜
pip install lm-eval datasets tiktoken openai psutil botocore

# ìœ„í—˜í•œ íŒ¨í‚¤ì§€ëŠ” ì„¤ì¹˜í•˜ì§€ ì•ŠìŒ (ê¸°ì¡´ ë²„ì „ ì‚¬ìš©)
# - torch (Neuron torch ì‚¬ìš©)
# - transformers (ê¸°ì¡´ ë²„ì „ ì‚¬ìš©)
# - pydantic (vLLM í˜¸í™˜ ë²„ì „ ì‚¬ìš©)
# - pyarrow (ê¸°ì¡´ ë²„ì „ ì‚¬ìš©)
```

### 1. ëª¨ë¸ ì»´íŒŒì¼

```bash
cd benchmark/scripts

# Llama 3.1 70B ì»´íŒŒì¼ (Light: 3ê°œ ëª¨ë¸)
./compile_model.sh ../configs/llama31-70b.conf light

# Qwen3 8B ì»´íŒŒì¼ (Medium: 6ê°œ ëª¨ë¸)
./compile_model.sh ../configs/qwen3-8b.conf medium
```

**ì»´íŒŒì¼ ë ˆë²¨:**
- `light`: 3ê°œ ëª¨ë¸ (BS1, BS2, BS4 ê° 1ê°œ)
- `medium`: 6ê°œ ëª¨ë¸ (ê° ë°°ì¹˜ ì‚¬ì´ì¦ˆë³„ 2ê°œ)
- `heavy`: 9ê°œ ëª¨ë¸ (ê° ë°°ì¹˜ ì‚¬ì´ì¦ˆë³„ 3ê°œ)

### 2. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

#### Performance Tests

##### vLLM Bench (ë¹ ë¥¸ ê²€ì¦)

```bash
# Llama 3.1 70B ë²¤ì¹˜ë§ˆí¬ (Light: 6ê°œ í…ŒìŠ¤íŠ¸)
./run_benchmark.sh ../configs/llama31-70b.conf light

# Qwen3 32B ë²¤ì¹˜ë§ˆí¬ (Medium: 15ê°œ í…ŒìŠ¤íŠ¸)
./run_benchmark.sh ../configs/qwen3-32b.conf medium
```

**í…ŒìŠ¤íŠ¸ ë ˆë²¨:**
- `light`: 6ê°œ í…ŒìŠ¤íŠ¸ (~30-40ë¶„)
- `medium`: 15ê°œ í…ŒìŠ¤íŠ¸ (~1-2ì‹œê°„)
- `heavy`: 30ê°œ í…ŒìŠ¤íŠ¸ (~2-4ì‹œê°„)

#### LLMPerf (ìƒì„¸ ë¶„ì„)

```bash
# Llama 3.1 70B LLMPerf (Light: 9ê°œ í…ŒìŠ¤íŠ¸)
./run_llmperf.sh ../configs/llama31-70b.conf light

# Qwen3 32B LLMPerf (Medium: 25ê°œ í…ŒìŠ¤íŠ¸)
./run_llmperf.sh ../configs/qwen3-32b.conf medium
```

**LLMPerf íŠ¹ì§•:**
- ì •ê·œë¶„í¬ ê¸°ë°˜ ì‹¤ì œ ì›Œí¬ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜
- P50, P90, P95, P99 ìƒì„¸ ë©”íŠ¸ë¦­
- ë‹¤ì–‘í•œ ë™ì‹œì„± ë ˆë²¨ í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ ë ˆë²¨:**
- `light`: 9ê°œ í…ŒìŠ¤íŠ¸ (3 variations Ã— 3 concurrency)
- `medium`: 25ê°œ í…ŒìŠ¤íŠ¸ (5 variations Ã— 5 concurrency)
- `heavy`: 56ê°œ í…ŒìŠ¤íŠ¸ (7 variations Ã— 8 concurrency)

#### Accuracy Tests

```bash
# Llama 3.1 70B ì •í™•ë„ í…ŒìŠ¤íŠ¸ (Light: 2 datasets)
./run_accuracy.sh ../configs/llama31-70b.conf light

# Qwen3 8B ì •í™•ë„ í…ŒìŠ¤íŠ¸ (Medium: 4 datasets)
./run_accuracy.sh ../configs/qwen3-8b.conf medium
```

**Accuracy íŠ¹ì§•:**
- AWS Neuron ê³µì‹ accuracy.py ì‚¬ìš© (aws-neuron-samples)
- lm-eval ê¸°ë°˜ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬
- MMLU, GSM8K, HellaSwag, ARC ë“±
- ìë™ ì„œë²„ ê´€ë¦¬ ë° ê²°ê³¼ ìˆ˜ì§‘

**ì‚¬ì „ ì¤€ë¹„:**
```bash
# ì²« ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ í´ë¡ ë˜ê³  ì•ˆì „í•œ ì˜ì¡´ì„±ë§Œ ì„¤ì¹˜ë¨
# ë˜ëŠ” ìˆ˜ë™ ì„¤ì¹˜:
git clone --depth 1 https://github.com/aws-neuron/aws-neuron-samples.git ~/aws-neuron-samples
pip install lm-eval datasets tiktoken openai psutil botocore
```

**âš ï¸ ì˜ì¡´ì„± ì¶©ëŒ ì£¼ì˜:**
- requirements.txtì˜ torch, transformers, pydanticëŠ” ì„¤ì¹˜í•˜ì§€ ë§ˆì„¸ìš”
- ê¸°ì¡´ vLLM í™˜ê²½ê³¼ ì¶©ëŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ ì•ˆì „í•œ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜í•©ë‹ˆë‹¤

**í…ŒìŠ¤íŠ¸ ë ˆë²¨:**
- `light`: 2ê°œ ë°ì´í„°ì…‹ (MMLU 100ìƒ˜í”Œ, GSM8K 50ìƒ˜í”Œ) - ~10-15ë¶„
- `medium`: 4ê°œ ë°ì´í„°ì…‹ (ê° 200-500ìƒ˜í”Œ) - ~30-60ë¶„
- `heavy`: 6ê°œ ë°ì´í„°ì…‹ (ì „ì²´ ë°ì´í„°ì…‹) - ~1-2ì‹œê°„

### 3. ë¦¬í¬íŠ¸ ìƒì„±

```bash
cd benchmark/reports

# Performance ê²°ê³¼ ë¦¬í¬íŠ¸ (vLLM Bench)
python3 generate_report.py ../scripts/benchmark_results/20260211_040816_light_llama-3-1-70b-instruct
python3 generate_html_report.py ../scripts/benchmark_results/20260211_040816_light_llama-3-1-70b-instruct

# Performance ê²°ê³¼ ë¦¬í¬íŠ¸ (LLMPerf)
python3 generate_report.py ../scripts/llmperf_results/20260211_050123_light_llama-3-1-70b-instruct
python3 generate_html_report.py ../scripts/llmperf_results/20260211_050123_light_llama-3-1-70b-instruct

# Accuracy ê²°ê³¼ ë¦¬í¬íŠ¸
python3 generate_report.py ../scripts/accuracy_results/20260211_060123_light_llama-3-1-70b-instruct
python3 generate_html_report.py ../scripts/accuracy_results/20260211_060123_light_llama-3-1-70b-instruct
```

## ğŸ” ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ ë¹„êµ

### Performance Tests

#### vLLM Bench
- âœ… ë¹ ë¥¸ ì‹¤í–‰ (ë‚´ì¥ ë„êµ¬)
- âœ… ê°„ë‹¨í•œ ì„¤ì •
- âœ… ê¸°ë³¸ ë©”íŠ¸ë¦­ (Throughput, TTFT, TPOT)
- ğŸ“Š ê³ ì • ì…ë ¥/ì¶œë ¥ ê¸¸ì´

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:** ë¹ ë¥¸ ì„±ëŠ¥ ê²€ì¦, ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë¹„êµ

#### LLMPerf
- âœ… ìƒì„¸í•œ ë©”íŠ¸ë¦­ (P50, P90, P95, P99)
- âœ… ì‹¤ì œ ì›Œí¬ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜ (ì •ê·œë¶„í¬)
- âœ… ë‹¤ì–‘í•œ ë™ì‹œì„± ë ˆë²¨
- ğŸ“Š í†µê³„ì  ë¶„ì„

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:** í”„ë¡œë•ì…˜ í‰ê°€, ìƒì„¸ ì„±ëŠ¥ ë¶„ì„

### Accuracy Tests

#### AWS Neuron Accuracy Suite
- âœ… ê³µì‹ AWS Neuron ë„êµ¬
- âœ… lm-eval ê¸°ë°˜ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬
- âœ… ìë™ ì„œë²„ ê´€ë¦¬
- ğŸ“Š MMLU, GSM8K, HellaSwag, ARC ë“±

**ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:** ëª¨ë¸ ì •í™•ë„ ê²€ì¦, ì»´íŒŒì¼ ì˜í–¥ í‰ê°€

### ê¶Œì¥ ì›Œí¬í”Œë¡œìš°

```bash
# 1. ëª¨ë¸ ì»´íŒŒì¼
./compile_model.sh ../configs/llama31-70b.conf light

# 2. ë¹ ë¥¸ ì„±ëŠ¥ ê²€ì¦ (vLLM Bench)
./run_benchmark.sh ../configs/llama31-70b.conf light

# 3. ì •í™•ë„ ê²€ì¦ (Accuracy)
./run_accuracy.sh ../configs/llama31-70b.conf light

# 4. ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ (LLMPerf) - ì„ íƒì‚¬í•­
./run_llmperf.sh ../configs/llama31-70b.conf light

# 5. ê²°ê³¼ í™•ì¸
ls -lh benchmark_results/
ls -lh accuracy_results/
ls -lh llmperf_results/
```

## âš™ï¸ ìƒˆ ëª¨ë¸ ì¶”ê°€

### 1. ì„¤ì • íŒŒì¼ ìƒì„±

`configs/your-model.conf` íŒŒì¼ì„ ìƒì„±:

```bash
# Model Information
MODEL_NAME="Your-Model-Name"
MODEL_PATH="/home/ubuntu/models/Your-Model/"
MODEL_TYPE="llama"  # llama, qwen2, mistral ë“±
TASK_TYPE="causal-lm"

# Compilation Settings
TORCH_DTYPE="bfloat16"
TP_DEGREE=64
PAD_TOKEN_ID=2

# Batch Size Configurations
BS1_CONFIG="12288 12800"  # CONTEXT_LENGTH SEQ_LENGTH
BS2_CONFIG="8192 8704"
BS4_CONFIG="4096 4608"

# Bucketing Configuration
BS1_CONTEXT_BUCKETS="2048 4096 8192 12288"
BS1_TOKEN_BUCKETS="2048 4096 8192 12800"
BS2_CONTEXT_BUCKETS="2048 4096 8192"
BS2_TOKEN_BUCKETS="2048 4096 8192 8704"
BS4_CONTEXT_BUCKETS="2048 4096"
BS4_TOKEN_BUCKETS="2048 4096 4608"

# Compilation Options
COMPILE_OPTS="--on-device-sampling \
--top-k 1 \
--do-sample \
--fused-qkv \
--sequence-parallel-enabled \
--qkv-kernel-enabled \
--attn-kernel-enabled \
--mlp-kernel-enabled \
--cc-pipeline-tiling-factor 1 \
--enable-bucketing"

# Neuron Runtime Settings
NEURON_RT_VIRTUAL_CORE_SIZE=2
NEURON_RT_NUM_CORES=64
NEURON_RT_EXEC_TIMEOUT=1800
XLA_DENSE_GATHER_FACTOR=0
NEURON_RT_INSPECT_ENABLE=0

# vLLM Server Settings
VLLM_BLOCK_SIZE=16
VLLM_RPC_TIMEOUT=100000

# Accuracy Test Settings
ACCURACY_LIGHT_TESTS=(
    "mmlu:100"
    "gsm8k:50"
)

ACCURACY_MEDIUM_TESTS=(
    "mmlu:500"
    "gsm8k:200"
    "hellaswag:500"
    "arc_challenge:200"
)

ACCURACY_HEAVY_TESTS=(
    "mmlu:0"
    "gsm8k:0"
    "hellaswag:0"
    "arc_challenge:0"
    "truthfulqa:0"
    "winogrande:0"
)

ACCURACY_MAX_CONCURRENT_REQUESTS=1
ACCURACY_TIMEOUT=3600
ACCURACY_SERVER_PORT=8000
ACCURACY_N_VLLM_THREADS=16
ACCURACY_CLIENT_PARAMS_BATCH_SIZE=1
ACCURACY_CLIENT_PARAMS_NUM_FEW_SHOT=5
```

### 2. ì»´íŒŒì¼ ë° ë²¤ì¹˜ë§ˆí¬

```bash
cd benchmark/scripts

# ì»´íŒŒì¼
./compile_model.sh ../configs/your-model.conf light

# Performance ë²¤ì¹˜ë§ˆí¬
./run_benchmark.sh ../configs/your-model.conf light

# Accuracy í…ŒìŠ¤íŠ¸
./run_accuracy.sh ../configs/your-model.conf light
```

## ğŸ“Š ê²°ê³¼ íŒŒì¼

ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í›„ ìƒì„±ë˜ëŠ” íŒŒì¼ë“¤:

### Performance Results (vLLM Bench / LLMPerf)
```
benchmark_results/20260211_040816_light_llama-3-1-70b-instruct/
â”œâ”€â”€ test_metadata.json                  # ì „ì²´ í…ŒìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ summary_light.csv                   # CSV ìš”ì•½
â”œâ”€â”€ failures.log                        # ì‹¤íŒ¨ ë¡œê·¸
â”œâ”€â”€ result_*.json                       # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
â”œâ”€â”€ benchmark_*.json                    # vLLM ë²¤ì¹˜ë§ˆí¬ ì›ë³¸
â”œâ”€â”€ benchmark_*.log                     # ë²¤ì¹˜ë§ˆí¬ ë¡œê·¸
â””â”€â”€ server_*.log                        # ì„œë²„ ë¡œê·¸
```

### Accuracy Results
```
accuracy_results/20260211_060123_light_llama-3-1-70b-instruct/
â”œâ”€â”€ test_metadata.json                  # ì „ì²´ í…ŒìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ summary_light.csv                   # CSV ìš”ì•½
â”œâ”€â”€ result_*.json                       # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
â”œâ”€â”€ config_*.yaml                       # í…ŒìŠ¤íŠ¸ë³„ ì„¤ì • íŒŒì¼
â””â”€â”€ accuracy_*.log                      # ì •í™•ë„ í…ŒìŠ¤íŠ¸ ë¡œê·¸
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰

```bash
# nohupìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup ./run_benchmark.sh ../configs/llama31-70b.conf light > benchmark.log 2>&1 &

# ì§„í–‰ ìƒí™© í™•ì¸
tail -f benchmark.log
```

### íŠ¹ì • ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œ ì»´íŒŒì¼

ì„¤ì • íŒŒì¼ì„ ìˆ˜ì •í•˜ì—¬ ì›í•˜ëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œ ì„¤ì •:

```bash
# configs/llama31-70b-bs1-only.conf
BS1_CONFIG="12288 12800"
BS2_CONFIG=""  # ë¹„í™œì„±í™”
BS4_CONFIG=""  # ë¹„í™œì„±í™”
```

### ê²°ê³¼ ë¹„êµ

ì—¬ëŸ¬ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë¹„êµ:

```bash
# Llama ë²¤ì¹˜ë§ˆí¬
./run_benchmark.sh ../configs/llama31-70b.conf light

# Qwen ë²¤ì¹˜ë§ˆí¬
./run_benchmark.sh ../configs/qwen3-32b.conf light

# ê²°ê³¼ ë¹„êµ
ls -lh benchmark_results/
```

## ğŸ“ ì£¼ì˜ì‚¬í•­

1. **ì»´íŒŒì¼ ì‹œê°„**: ì²« ì»´íŒŒì¼ì€ 20-60ë¶„ ì†Œìš”
2. **ë””ìŠ¤í¬ ê³µê°„**: ì»´íŒŒì¼ëœ ëª¨ë¸ë‹¹ ìˆ˜ GB í•„ìš”
3. **ë©”ëª¨ë¦¬**: ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ í´ìˆ˜ë¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
4. **ë™ì‹œ ì‹¤í–‰ ê¸ˆì§€**: í•œ ë²ˆì— í•˜ë‚˜ì˜ ë²¤ì¹˜ë§ˆí¬ë§Œ ì‹¤í–‰
5. **Accuracy í…ŒìŠ¤íŠ¸**: aws-neuron-samples ë¦¬í¬ì§€í† ë¦¬ í•„ìš” (ìë™ í´ë¡ ë¨)
6. **ì˜ì¡´ì„± ì¶©ëŒ**: requirements.txtì˜ ì¼ë¶€ íŒ¨í‚¤ì§€ëŠ” ê¸°ì¡´ í™˜ê²½ê³¼ ì¶©ëŒ ê°€ëŠ¥

### ì˜ì¡´ì„± ê´€ë¦¬

**ì•ˆì „í•œ íŒ¨í‚¤ì§€ (ì„¤ì¹˜ ê¶Œì¥):**
```bash
pip install lm-eval datasets tiktoken openai psutil botocore
```

**ìœ„í—˜í•œ íŒ¨í‚¤ì§€ (ì„¤ì¹˜ ê¸ˆì§€):**
- `torch` - Neuron torchì™€ ì¶©ëŒ
- `transformers` - ë²„ì „ ì¶©ëŒ ê°€ëŠ¥
- `pydantic>2.10` - vLLM í˜¸í™˜ì„± ë¬¸ì œ
- `pyarrow==20.0.0` - ë²„ì „ ê³ ì •ìœ¼ë¡œ ì¶©ëŒ

**í™•ì¸ ë°©ë²•:**
```bash
# í˜„ì¬ ì„¤ì¹˜ëœ ë²„ì „ í™•ì¸
pip list | grep -E "torch|transformers|pydantic|pyarrow"

# lm-eval ì„¤ì¹˜ í™•ì¸
python3 -c "import lm_eval; print('lm-eval:', lm_eval.__version__)"
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ì»´íŒŒì¼ ì‹¤íŒ¨

```bash
# ë¡œê·¸ í™•ì¸
cat /home/ubuntu/compiled_models/llama31-70b-bs1-ctx12288/compile.log

# íƒ€ì„ì•„ì›ƒ ì¦ê°€
# configs/*.conf íŒŒì¼ì—ì„œ:
NEURON_RT_EXEC_TIMEOUT=3600  # 60ë¶„ìœ¼ë¡œ ì¦ê°€
```

### ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨

```bash
# ì‹¤íŒ¨ ë¡œê·¸ í™•ì¸
cat benchmark_results/*/failures.log

# ì„œë²„ ë¡œê·¸ í™•ì¸
cat benchmark_results/*/server_*.log
```

### ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨

```bash
cd benchmark_results/20260211_040816_light_llama-3-1-70b-instruct/

# ìˆ˜ë™ ì—…ë°ì´íŠ¸
python3 << 'EOF'
import json, glob
from datetime import datetime

with open('test_metadata.json', 'r') as f:
    metadata = json.load(f)

metadata['tests'] = []
for result_file in sorted(glob.glob('result_*.json')):
    with open(result_file, 'r') as f:
        metadata['tests'].append(json.load(f))

metadata['end_time'] = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
metadata['total_tests'] = len(metadata['tests'])
metadata['successful_tests'] = sum(1 for t in metadata['tests'] if t['status'] == 'SUCCESS')
metadata['failed_tests'] = sum(1 for t in metadata['tests'] if t['status'] == 'FAILED')

with open('test_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
EOF
```

### Accuracy ìŠ¤í¬ë¦½íŠ¸ ì°¾ê¸° ì‹¤íŒ¨

```bash
# aws-neuron-samples í™•ì¸
ls -la ~/aws-neuron-samples/inference-benchmarking/accuracy.py

# ì—†ìœ¼ë©´ í´ë¡ 
git clone --depth 1 https://github.com/aws-neuron/aws-neuron-samples.git ~/aws-neuron-samples

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r ~/aws-neuron-samples/inference-benchmarking/requirements.txt

# ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ í´ë¡ í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŒ
# ì²« ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
```

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [Llama 3.1 70B ê°€ì´ë“œ](../inference/Llama3.1-70B-Instruct-Trn2.md)
- [Qwen3 32B ê°€ì´ë“œ](../inference/Qwen3-32B-Dense-BF16-Trn2.md)
- [AWS Neuron ê³µì‹ ë¬¸ì„œ](https://awsdocs-neuron.readthedocs-hosted.com/)
- [AWS Neuron Accuracy Evaluation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/accuracy-eval-with-datasets.html)
- [lm-eval Documentation](https://github.com/EleutherAI/lm-evaluation-harness)
