# Qwen3 8B Instruct Configuration

# ============================================
# Common Settings (Used by all tools)
# ============================================
MODEL_NAME="Qwen3-8B-Instruct"
MODEL_PATH="/home/ubuntu/models/Qwen3-8B/"
MODEL_TYPE="qwen2"
TASK_TYPE="causal-lm"

# Compilation Settings
TORCH_DTYPE="bfloat16"
TP_DEGREE=32
PAD_TOKEN_ID=151643

# Batch Size Configurations
# Format: CONTEXT_LENGTH SEQ_LENGTH
BS1_CONFIG="16384 16896"
BS2_CONFIG="16384 16896"
BS4_CONFIG="8192 8704"

# Bucketing Configuration
# BS1 & BS2
BS1_CONTEXT_BUCKETS="2048 4096 8192 16384"
BS1_TOKEN_BUCKETS="2048 4096 8192 16384 16896"
BS2_CONTEXT_BUCKETS="2048 4096 8192 16384"
BS2_TOKEN_BUCKETS="2048 4096 8192 16384 16896"

# BS4
BS4_CONTEXT_BUCKETS="2048 4096 8192"
BS4_TOKEN_BUCKETS="2048 4096 8192 8704"

# Compilation Options
COMPILE_OPTS="--on-device-sampling \
--top-k 20 \
--do-sample \
--fused-qkv \
--sequence-parallel-enabled \
--qkv-kernel-enabled \
--attn-kernel-enabled \
--mlp-kernel-enabled \
--cc-pipeline-tiling-factor 1 \
--enable-bucketing"

# Neuron Runtime Settings
NEURON_RT_VIRTUAL_CORE_SIZE=2
NEURON_RT_NUM_CORES=64
NEURON_RT_EXEC_TIMEOUT=1800
XLA_DENSE_GATHER_FACTOR=0
NEURON_RT_INSPECT_ENABLE=0

# vLLM Server Settings
VLLM_BLOCK_SIZE=16
VLLM_RPC_TIMEOUT=100000
VLLM_EXTRA_ARGS=""

# ============================================
# LLMPerf Settings (Used by run_llmperf.sh)
# ============================================

# Request Distribution (Normal Distribution)
LLMPERF_MEAN_INPUT_TOKENS=550
LLMPERF_STDDEV_INPUT_TOKENS=150
LLMPERF_MEAN_OUTPUT_TOKENS=150
LLMPERF_STDDEV_OUTPUT_TOKENS=10

# Test Configuration
LLMPERF_MAX_REQUESTS=500
LLMPERF_TIMEOUT=600
LLMPERF_NUM_CONCURRENT_REQUESTS=1

# API Settings
LLMPERF_API_TYPE="openai"
LLMPERF_API_BASE="http://localhost:8000/v1"

# Test Levels
LLMPERF_LIGHT_CONCURRENCY=(1 2 4 8)
LLMPERF_MEDIUM_CONCURRENCY=(1 2 4 8 16 32)
LLMPERF_HEAVY_CONCURRENCY=(1 2 4 8 16 32 64 128)

# Input/Output Length Variations
LLMPERF_LIGHT_VARIATIONS=(
    "256 50 64 10"
    "550 150 150 10"
    "1024 200 256 20"
)

LLMPERF_MEDIUM_VARIATIONS=(
    "128 30 32 5"
    "256 50 64 10"
    "550 150 150 10"
    "1024 200 256 20"
    "2048 300 512 30"
)

LLMPERF_HEAVY_VARIATIONS=(
    "128 30 32 5"
    "256 50 64 10"
    "512 100 128 15"
    "550 150 150 10"
    "1024 200 256 20"
    "2048 300 512 30"
    "4096 500 1024 50"
)

# Additional Sampling Parameters (JSON format)
LLMPERF_SAMPLING_PARAMS='{
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 512
}'

# ============================================
# Accuracy Test Settings (Used by run_accuracy.sh)
# ============================================

# Accuracy Test Levels
ACCURACY_LIGHT_TESTS=(
    "mmlu:100"
    "gsm8k:50"
)

ACCURACY_MEDIUM_TESTS=(
    "mmlu:500"
    "gsm8k:200"
    "hellaswag:500"
    "arc_challenge:200"
)

ACCURACY_HEAVY_TESTS=(
    "mmlu:0"
    "gsm8k:0"
    "hellaswag:0"
    "arc_challenge:0"
    "truthfulqa:0"
    "winogrande:0"
)

# Accuracy Test Configuration
ACCURACY_MAX_CONCURRENT_REQUESTS=1
ACCURACY_TIMEOUT=3600
ACCURACY_SERVER_PORT=8000
ACCURACY_N_VLLM_THREADS=16

# LM-Eval Client Parameters
ACCURACY_CLIENT_PARAMS_BATCH_SIZE=1
ACCURACY_CLIENT_PARAMS_NUM_FEW_SHOT=5