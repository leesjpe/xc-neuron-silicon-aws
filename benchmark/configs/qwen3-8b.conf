# Model Information
MODEL_NAME="qwen3-8b"
MODEL_PATH="/home/ubuntu/models/Qwen3-8B"
MODEL_TYPE="qwen3"
TASK_TYPE="causal-lm"

# Compilation Settings
TORCH_DTYPE="bfloat16"
PAD_TOKEN_ID=151643

# --- Test Matrix ---
# Combinations of TP_DEGREES and BATCH_SIZES will be compiled.
# NOTE: Based on a failure at tp32-bs8 with 16k context, we are reducing the context length
# to 8k to find the new max batch size.
TP_DEGREES="32 64"
BATCH_SIZES="1 2 4 8 16 32 64 128"

# Sequence and Context Length
# Reduced from 16384 to 8192 to save memory and allow for higher batch sizes.
MAX_CONTEXT_LENGTH=8192
SEQ_LEN=8192

# Bucketing Configuration
CONTEXT_BUCKETS="256 512 1024 2048 4096 8192"
TOKEN_BUCKETS="256 512 1024 2048 4096 8192"

# Compilation Options (based on successful Qwen3-8B compilation)
COMPILE_OPTS="--on-device-sampling \
--do-sample \
--sequence-parallel-enabled \
--attn-kernel-enabled \
--mlp-kernel-enabled \
--cc-pipeline-tiling-factor 1 \
--enable-bucketing"

# Neuron Runtime Settings
NEURON_RT_VIRTUAL_CORE_SIZE=2
# For trn2.48xlarge with 128 physical cores, this results in 64 logical cores
NEURON_RT_NUM_CORES=$((128 / NEURON_RT_VIRTUAL_CORE_SIZE))
NEURON_RT_EXEC_TIMEOUT=1800
XLA_DENSE_GATHER_FACTOR=0
NEURON_RT_INSPECT_ENABLE=0

# --- VLLM Benchmark Settings (for run_vllm_bench.sh) ---
VLLM_BENCH_CONCURRENCIES="1 2 4 8 16 32 64 128"
VLLM_BENCH_INPUT_LEN=1024
VLLM_BENCH_OUTPUT_LEN=256
VLLM_BENCH_NUM_PROMPTS=256

# --- Accuracy Test Settings (for run_accuracy.sh) ---
# Format: "dataset_name:sample_limit" (0 for full dataset)
ACCURACY_DATASETS="hellaswag:100 piqa:100 mmlu:50"
ACCURACY_MAX_CONCURRENT_REQUESTS=1
ACCURACY_TIMEOUT=3600
ACCURACY_SERVER_PORT=8000
ACCURACY_N_VLLM_THREADS=16
ACCURACY_CLIENT_PARAMS_BATCH_SIZE=1
ACCURACY_CLIENT_PARAMS_NUM_FEW_SHOT=0 # Set to 0 for most zero-shot tasks like hellaswag

# --- LLMPerf Settings (for run_llmperf.sh) ---
LLMPERF_CONCURRENCIES="1 2 4 8 16 32 64 128"
# Format: "mean_input std_input mean_output std_output"
LLMPERF_VARIATIONS="1024 0 256 0"
LLMPERF_MAX_REQUESTS=256
LLMPERF_TIMEOUT=1800
LLMPERF_API_TYPE="openai"
LLMPERF_API_BASE="http://localhost:8000/v1"
LLMPERF_SAMPLING_PARAMS='{"temperature": 0.0}'