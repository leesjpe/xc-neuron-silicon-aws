# Serving Qwen3 30B BF16 on Trainium2 with vLLM (NxD)

ì´ ê°€ì´ë“œëŠ” **AWS Trainium2 (`trn2.48xlarge`)** ì¸ìŠ¤í„´ìŠ¤ì—ì„œ **vLLM**ê³¼ **Neuronx Distributed (NxD)** ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ **Qwen 3 (32B)** ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
aws neuron ê³µì‹ë¬¸ì„œì˜ [Tutorial](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/sd-inference-tutorial.html) ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„± ë˜ì—ˆìŠµë‹ˆë‹¤. 

**Quickstart using Docker** vLLMì´ ì‚¬ì „ ì„¤ì¹˜ëœ AWS Neuron í¬í¬ ë²„ì „ì˜ ì‚¬ì „ êµ¬ì„±ëœ ë”¥ ëŸ¬ë‹ ì»¨í…Œì´ë„ˆ(DLC)ë¥¼ í™œìš©í•©ë‹ˆë‹¤. 

* aws-neuron Github ì˜ [deep-learning-contianers](https://github.com/aws-neuron/deep-learning-containers?tab=readme-ov-file#vllm-inference-neuronx) ì˜ vllm-inference-neuronx ì—ì„œ ì»¨í…Œì´ë„ˆ ë³„ vLLM Framework ë²„ì „, Neuron SDK ë²„ì „, ECR Public URLì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

* vLLM V1 ë°©ì‹ì„ ë”°ë¥´ë©° [vllm-project/vllm-neuron](https://github.com/vllm-project/vllm-neuron) ê¸°ë°˜ìœ¼ë¡œ vLLM ì„œë²„ë¥¼ ë°°í¬í•©ë‹ˆë‹¤. ([Neuron SDK 2.27](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/2.27.0/index.html) ë²„ì „ ì´ìƒë¶€í„° ì ìš© ê°€ëŠ¥)

* ê¸°ì¡´ vLLM V0 ë°©ì‹ì€ Neuron SDK 2.28 ì—ì„œ Deprecated ë  ì˜ˆì •ì…ë‹ˆë‹¤. [Inference update](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/about-neuron/whats-new.html#id6)ì—ì„œ ê³µì§€. 
V0ë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´ V1 ìœ¼ë¡œì˜ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ê³ ë ¤í•˜ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤.  

---
### ğŸ“‹ Prerequisites (ì‚¬ì „ ì¤€ë¹„)

ì§„í–‰í•˜ê¸° ì „ì— ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•˜ì„¸ìš”.

1.  **ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰:** `trn2.48xlarge` ì¸ìŠ¤í„´ìŠ¤ê°€ í™œì„±í™”(`Running`) ìƒíƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
    * ğŸ‘‰ **[ê°€ì´ë“œ: Capacity Block ê¸°ë°˜ XC ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰](https://github.com/leesjpe/compute-foundation-on-aws/blob/main/ec2/ec2-dlami-neuron.md)** 
3.  **(ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ê¶Œì¥) ê³ ì† ìŠ¤í† ë¦¬ì§€ ì„¤ì •:**
    * ëª¨ë¸ ë¡œë”© ì†ë„ì™€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë¡œì»¬ NVMe SSD (RAID 0) ì‚¬ìš©ì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤.
    * ì•„ì§ ì„¤ì •í•˜ì§€ ì•Šìœ¼ì…¨ë‹¤ë©´, ì•„ë˜ ê°€ì´ë“œë¥¼ ë¨¼ì € ì§„í–‰í•´ ì£¼ì„¸ìš”.
    * ğŸ‘‰ **[ê°€ì´ë“œ: ê³ ì† ìŠ¤í† ë¦¬ì§€ ì„¤ì • (NVMe RAID 0)](https://github.com/leesjpe/compute-foundation-on-aws/blob/main/storage/local-nvme-setup.md)**
    * *ì°¸ê³ : ì´ ê³¼ì •ì„ ê±´ë„ˆë›´ë‹¤ë©´, ë£¨íŠ¸ EBS ë³¼ë¥¨ì— ëª¨ë¸ì„ ì €ì¥í•  ì¶©ë¶„í•œ ê³µê°„ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.*

---

### 1. ğŸ³ Docker ê¸°ë°˜ vLLM ì„œë²„ ë°°í¬
#### Step 1-1: Neuron vLLM ì»¨í…Œì´ë„ˆ ì‹¤í–‰

ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ AWS ê³µì‹ **Neuron Deep Learning Container (DLC)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰í•˜ê³  ë‚´ë¶€ ì‰˜ë¡œ ì§„ì…í•©ë‹ˆë‹¤.
*(ê³ ì† ìŠ¤í† ë¦¬ì§€ë¥¼ ì„¤ì •í–ˆë‹¤ë©´ `/data` ë§ˆìš´íŠ¸ê°€ ê°€ëŠ¥í•˜ë©°, EBSë§Œ ì‚¬ìš©í•œë‹¤ë©´ ê²½ë¡œëŠ” ìƒí™©ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.)*

```bash
# Docker ì‹¤í–‰ ë° ì§„ì…
# -v /data:/data : ê³ ì† ìŠ¤í† ë¦¬ì§€(ë˜ëŠ” ëª¨ë¸ ê²½ë¡œ) ë§ˆìš´íŠ¸
# -p 8000:8000 : API ì„œë²„ í¬íŠ¸ ê°œë°©

docker run -d -it \
  --privileged \
  -v /home/ubuntu/:/home/ubuntu/ \
  -v /data:/data \
  -p 8000:8000 \
  public.ecr.aws/neuron/pytorch-inference-vllm-neuronx:0.11.0-neuronx-py312-sdk2.27.0-ubuntu24.04
```

#### ğŸ“¥ Step 1-2: ê°€ìƒí™˜ê²½ ì§„ì…

```bash
docker exec -it <Container ID> bash
```

#### âš™ï¸ Step 1-3 vllm server ì‹¤í–‰

1-3: Model download

```bash
mkdir /home/ubuntu/qwen3_32b_dense_bf16
hf download Qwen/Qwen3-32B --local-dir /home/ubuntu/qwen3_32b_dense_bf16
```

1-3 ëŠ” BF16 ê¸°ì¤€ì˜ [Qwen3 32B Model](https://huggingface.co/Qwen/Qwen3-32B) ì¶”ë¡ ì„ ìœ„í•œ vllm ì„œë²„ë¥¼ ì‹¤í–‰ í•©ë‹ˆë‹¤. 

1-3 ê³¼ì •ì€  10~15 ì†Œìš”ë˜ë©° ì•„ë˜ì™€ ê°™ì´ ë¡œê·¸ê°€ ë³´ì´ë©´ ì»´íŒŒì¼ ë° ì„œë²„ ì‹œì‘ ì™„ë£Œ â˜•ï¸

ê¸°ì¡´ vLLM V0 ë°©ì‹ì—ì„œëŠ” VLLM_USE_V1=0 ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ V1 ì—ì„œëŠ” ì œì™¸ í•©ë‹ˆë‹¤.

```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export VLLM_NEURON_FRAMEWORK="neuronx-distributed-inference"
export NEURON_COMPILED_ARTIFACTS="/home/ubuntu/qwen3_32b_dense_bf16_artifacts"
export MODEL_ID="/home/ubuntu/qwen3_32b_dense_bf16"

# ì„œë²„ ì‹¤í–‰ (8000 í¬íŠ¸ í†µí•œ ì™¸ë¶€ ì ‘ì† í—ˆìš©, íŠ¹ì • IP ë¡œ ì œí•œ ê¶Œì¥)
vllm serve $MODEL_ID \
    --tensor-parallel-size 16 \
    --max-num-seqs 32 \
    --max-model-len 4096 \
    --block-size 32 \
    --host 0.0.0.0 \
    --port 8000
```
<img width="1294" height="545" alt="Screenshot 2025-12-06 at 9 17 48â€¯PM" src="https://github.com/user-attachments/assets/4cf45802-3e9a-4290-b0c0-e5303f384e40" />

vLLM Model ID í™•ì¸
```bash
curl http://localhost:8000/v1/models
```

#### ğŸ§ª Step 1-4: ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Inference)

**Host Machine**ì—ì„œ ì»¨í…Œì´ë„ˆì—ì„œ ì‹¤í–‰ ì¤‘ì¸ vLLM ì„œë²„ë¡œì˜ ì¶”ë¡  í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```bash
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
"model": "/home/ubuntu/qwen3_32b_dense_bf16",
"prompt": "What is machine learning?",
"max_tokens": 100,
"temperature": 0.7
}'
```
<img width="1295" height="216" alt="Screenshot 2025-12-06 at 10 38 58â€¯PM" src="https://github.com/user-attachments/assets/451cf358-9bc8-45ab-bc77-fb491cb57a6d" />

#### ğŸ“Š Step 1-5: Performance Benchmarking (via Host)

ì´ ê°€ì´ë“œì—ì„œëŠ” **Host Machine**ì—ì„œ vLLM ì„œë²„ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. í˜¸ìŠ¤íŠ¸ì—ì„œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•˜ë©´ Python ë²„ì „ ì¶©ëŒì„ ë°©ì§€í•˜ê³  ë¦¬ì†ŒìŠ¤ ë¶„ë¦¬ë¥¼ â€‹â€‹ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

##### Prerequisites
* `llmperf` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” **Python 3.8 ~ 3.10** í™˜ê²½ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ê¸°ë³¸ Python ë²„ì „ì´ ë„ˆë¬´ ë†’ê±°ë‚˜(3.11+), íŒ¨í‚¤ì§€ê°€ ê¼¬ì´ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ `conda` ê°€ìƒí™˜ê²½ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

ë³¸ ê°€ì´ë“œëŠ” llmperf ë¥¼ ìœ„í•œ ë³„ë„ ê°€ìƒí™˜ê²½ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì‚¬ì „ ì¸ìŠ¤í†¨ëœ neuron ì˜ ê°€ìƒí™˜ê²½ì„ ì‚¬ìš©í•  ê²½ìš° llmperf ë¥¼ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤. 

```bash
# 1. llmperf ì†ŒìŠ¤ ì½”ë“œ ë‹¤ìš´ë¡œë“œ (git clone)
git clone https://github.com/ray-project/llmperf.git
cd llmperf

# 2. pyproject.toml íŒŒì¼ì—ì„œ Python ë²„ì „ ì œí•œ(<3.11)ì„ <3.13ìœ¼ë¡œ ìˆ˜ì • (sed ëª…ë ¹ì–´ ì‚¬ìš©)
sed -i 's/<3.11/<3.13/g' pyproject.toml

# 3. ìˆ˜ì •ëœ ì†ŒìŠ¤ë¡œ ì„¤ì¹˜ (-e ì˜µì…˜ ì‚¬ìš©)
pip install -e . --no-deps
```

* ë²„ì „ ìˆ˜ì •: llmperfì— ì„¤ì •ëœ íŒŒì´ì¬ ìƒí•œì„ ( <3.11)ì„ ì œê±°í•˜ì—¬ í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ 3.12.3 í™˜ê²½ì—ì„œ ì„¤ì¹˜ê°€ ê±°ë¶€ë˜ëŠ” ê²ƒì„ ë°©ì§€.
* --no-deps ì„¤ì •: ì´ë¯¸ vLLM í™˜ê²½ì— ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ë“¤ê³¼ llmperf ê°„ì˜ ë¶ˆí•„ìš”í•œ ë²„ì „ ì¶©ëŒ(Pydantic ë“±) ê²€ì‚¬ë¥¼ ê±´ë„ˆë›°ê³  ì„¤ì¹˜.

![alt text](<Screenshot 2026-01-14 at 3.23.54â€¯PM.png>)

ì„¤ì¹˜ ì™„ë£Œ í›„ì— ì•„ë˜ ëª…ë ¹ì–´ë¥¼ Host ì—ì„œ ì‹¤í–‰

```bash
export OPENAI_API_BASE="http://localhost:8000/v1"
export OPENAI_API_KEY=dummy

python token_benchmark_ray.py \
    --model "/home/ubuntu/qwen3_32b_dense_bf16" \
    --mean-input-tokens 128 \
    --stddev-input-tokens 0 \
    --mean-output-tokens 512 \
    --stddev-output-tokens 0 \
    --max-num-completed-requests 10 \
    --timeout 1200 \
    --num-concurrent-requests 1 \
    --results-dir /tmp/results \
    --llm-api openai \
    --additional-sampling-params '{}'

```

<img width="364" height="1172" alt="Screenshot 2025-12-08 at 9 12 32â€¯PM" src="https://github.com/user-attachments/assets/ce17baea-c904-42a4-bf9d-b848db455af5" />



## 2.ğŸ–¥ï¸ Host machine ì˜ ê°€ìƒí™˜ê²½ì„ í™œìš©í•œ vLLM ì„œë²„ ë°°í¬

### Step 2-1: vLLM V1 ê°€ìƒí™˜ê²½ í™œì„±í™” 
```bash
source /opt/aws_neuronx_venv_pytorch_inference_vllm/bin/activate
```

* ì´í›„ ì„œë²„ ì‹¤í–‰ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸, Benchmarking ê³¼ì •ì€ Docker ë°©ì‹ì—ì„œ ì•ˆë‚´ í•œ ë‚´ìš©ê³¼ ë™ì¼í•˜ê²Œ ì§„í–‰

[âš™ï¸ Step 1-3 vllm server ì‹¤í–‰](#ï¸-step-1-3-vllm-server-ì‹¤í–‰)

[ğŸ§ª Step 1-4: ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Inference)](#-step-1-4-ì¶”ë¡ -í…ŒìŠ¤íŠ¸-inference)

[ğŸ“Š Step 1-5: Performance Benchmarking (via Host)](#-step-1-5-performance-benchmarking-via-host)
