# Serving Qwen 3/2.5 on Trainium2 with vLLM (NxD)

ì´ ê°€ì´ë“œëŠ” **AWS Trainium2 (`trn2.48xlarge`)** ì¸ìŠ¤í„´ìŠ¤ì—ì„œ **vLLM**ê³¼ **Neuronx Distributed (NxD)** ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ **Qwen 3 (32B)** ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ê¸°ì¡´ Legacy ë°©ì‹ì´ ì•„ë‹Œ, **NxD ê¸°ë°˜ì˜ vLLM (v1 íŒŒì´í”„ë¼ì¸)**ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ëª¨ë¸ì— ëŒ€í•œ ì•ˆì •ì„±ê³¼ ìµœì í™”ëœ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

---

## ğŸ“‹ Prerequisites (ì‚¬ì „ ì¤€ë¹„)

ì§„í–‰í•˜ê¸° ì „ì— ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•˜ì„¸ìš”.

1.  **ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰:** `trn2.48xlarge` ì¸ìŠ¤í„´ìŠ¤ê°€ í™œì„±í™”(`Running`) ìƒíƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
    * ğŸ‘‰ **[ê°€ì´ë“œ: Capacity Block ê¸°ë°˜ XC ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰](https://github.com/leesjpe/compute-foundation-on-aws/blob/main/ec2/ec2-dlami-neuron.md)** 
3.  **(ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ê¶Œì¥) ê³ ì† ìŠ¤í† ë¦¬ì§€ ì„¤ì •:**
    * ëª¨ë¸ ë¡œë”© ì†ë„ì™€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë¡œì»¬ NVMe SSD (RAID 0) ì‚¬ìš©ì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤.
    * ì•„ì§ ì„¤ì •í•˜ì§€ ì•Šìœ¼ì…¨ë‹¤ë©´, ì•„ë˜ ê°€ì´ë“œë¥¼ ë¨¼ì € ì§„í–‰í•´ ì£¼ì„¸ìš”.
    * ğŸ‘‰ **[ê°€ì´ë“œ: ê³ ì† ìŠ¤í† ë¦¬ì§€ ì„¤ì • (NVMe RAID 0)](../storage/local-nvme-setup.md)**
    * *ì°¸ê³ : ì´ ê³¼ì •ì„ ê±´ë„ˆë›´ë‹¤ë©´, ë£¨íŠ¸ EBS ë³¼ë¥¨ì— ëª¨ë¸ì„ ì €ì¥í•  ì¶©ë¶„í•œ ê³µê°„ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.*

---

## ğŸ³ Step 1: Neuron vLLM ì»¨í…Œì´ë„ˆ ì‹¤í–‰

ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ AWS ê³µì‹ **Neuron Deep Learning Container (DLC)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰í•˜ê³  ë‚´ë¶€ ì‰˜ë¡œ ì§„ì…í•©ë‹ˆë‹¤.
*(ê³ ì† ìŠ¤í† ë¦¬ì§€ë¥¼ ì„¤ì •í–ˆë‹¤ë©´ `/data` ë§ˆìš´íŠ¸ê°€ í•„ìˆ˜ì´ë©°, EBSë§Œ ì‚¬ìš©í•œë‹¤ë©´ ê²½ë¡œëŠ” ìƒí™©ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.)*

```bash
# Docker ì‹¤í–‰ ë° ì§„ì…
# -v /data:/data : ê³ ì† ìŠ¤í† ë¦¬ì§€(ë˜ëŠ” ëª¨ë¸ ê²½ë¡œ) ë§ˆìš´íŠ¸
# -p 8000:8000 : API ì„œë²„ í¬íŠ¸ ê°œë°©

docker run -d -it \
  --privileged \
  -v /home/ubuntu/:/home/ubuntu/ \
  -v /data:/data \
  -p 8000:8000 \
  public.ecr.aws/neuron/pytorch-inference-vllm-neuronx:0.9.1-neuronx-py311-sdk2.26.1-ubuntu22.04
```

## ğŸ“¥ Step 2: ê°€ìƒí™˜ê²½ ì§„ì…

```bash
docker exec -it <Container ID> bash
```

## âš™ï¸ Step 3: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë° vllm server ì‹¤í–‰
3-2 ê³¼ì •ì€  10~15 ì†Œìš”ë˜ë©° ì•„ë˜ì™€ ê°™ì´ ë¡œê·¸ê°€ ë³´ì´ë©´ ì»´íŒŒì¼ ë° ì„œë²„ ì‹œì‘ ì™„ë£Œ â˜•ï¸

```bash
# 3-1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export VLLM_NEURON_FRAMEWORK="neuronx-distributed-inference"
export NEURON_COMPILED_ARTIFACTS="/data/Qwen-32B-BS1-SL6k-TP64"
export MODEL_ID="Qwen/Qwen3-32B"

# 3-2. ì„œë²„ ì‹¤í–‰ (ì™¸ë¶€ ì ‘ì† í—ˆìš©)
VLLM_USE_V1=0 vllm serve $MODEL_ID \
    --tensor-parallel-size 64 \
    --max-num-seqs 1 \
    --max-model-len 6400 \
    --override-neuron-config '{"save_sharded_checkpoint": true}' \
    --host 0.0.0.0 \
    --port 8000
```
<img width="1294" height="845" alt="Screenshot 2025-12-06 at 9 17 48â€¯PM" src="https://github.com/user-attachments/assets/4cf45802-3e9a-4290-b0c0-e5303f384e40" />


## ğŸ§ª Step 4: ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Inference)
```bash
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
"model": "Qwen/Qwen3-32B",
"prompt": "What is machine learning?",
"max_tokens": 100,
"temperature": 0.7
}'
```
<img width="1295" height="316" alt="Screenshot 2025-12-06 at 10 38 58â€¯PM" src="https://github.com/user-attachments/assets/451cf358-9bc8-45ab-bc77-fb491cb57a6d" />


