# Qwen3-Coder-480B Inference on AWS Trainium2 (Trn2) with vLLM

ì´ ê°€ì´ë“œëŠ” **AWS Trainium2 (Trn2)** ì¸ìŠ¤í„´ìŠ¤ì—ì„œ **vLLM**ì„ ì‚¬ìš©í•˜ì—¬ [Qwen3-Coder-480B-A35B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct) ëª¨ë¸ì„ ì„œë¹™í•˜ê³  ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ë³¸ ê°€ì´ë“œëŠ” MoE(Mixture of Experts) ì•„í‚¤í…ì²˜ë¥¼ ì§€ì›í•˜ë©°, Neuron SDKì— ìµœì í™”ëœ ì„¤ì •ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„ (Prerequisites)

ì´ ê°€ì´ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— **EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ìŠ¤í† ë¦¬ì§€ ì„¤ì •**ì´ ì™„ë£Œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
ëŒ€ìš©ëŸ‰ ëª¨ë¸(480B)ì„ ë‹¤ë£¨ê¸° ìœ„í•´ ê³ ì„±ëŠ¥ ìŠ¤í† ë¦¬ì§€ êµ¬ì„±ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

- **[EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±](https://github.com/leesjpe/compute-foundation-on-aws/blob/main/ec2/ec2-dlami-neuron.md) ë° [ê³ ì„±ëŠ¥ ìŠ¤í† ë¦¬ì§€(RAID 0) ë§ˆìš´íŠ¸](https://github.com/leesjpe/compute-foundation-on-aws/blob/main/storage/local-nvme-setup.md) ë°©ë²•** 
- **ê¶Œì¥ ì¸ìŠ¤í„´ìŠ¤:** `trn2.48xlarge`
- **í•„ìˆ˜ ìŠ¤í† ë¦¬ì§€:** `/data` ê²½ë¡œì— ìµœì†Œ 4TB ì´ìƒì˜ NVMe/EBS ë³¼ë¥¨ ë§ˆìš´íŠ¸

---

## ğŸ› ï¸ 1. í™˜ê²½ ì„¤ì • ë° ì¢…ì†ì„± í™•ì¸

AWS Neuron vLLM ê°€ìƒ í™˜ê²½ì„ í™œì„±í™”í•˜ê³  ë²„ì „ì„ í™•ì¸í•©ë‹ˆë‹¤.

```bash
# ê°€ìƒ í™˜ê²½ í™œì„±í™”
source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate

# ì£¼ìš” íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸
pip list | grep neuron
```

### âœ… Tested Environment Versions
ë³¸ ê°€ì´ë“œëŠ” ì•„ë˜ ë²„ì „ì—ì„œ Test ë˜ì—ˆìŠµë‹ˆë‹¤.

```
pip list | grep neuron
```

| Package Name | Version |
| :--- | :--- |
| `libneuronxla` | `2.2.14584.0+06ac23d1` |
| `neuronx-cc` | `2.22.12471.0+b4a00d10` |
| `neuronx-distributed` | `0.16.25997+f431c02e` |
| `neuronx-distributed-inference` | `0.7.15063+bafa28d5` |
| `torch-neuronx` | `2.9.0.2.11.19912+e48cd891` |
| `vllm-neuron` | `0.3.0` |

---

## ğŸ“¥ 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

Hugging Face CLIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ `/data` ë””ë ‰í„°ë¦¬ì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
ë‹¤ìš´ë¡œë“œ ê³¼ì •ì„ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•´ í¬ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

> **Note:** ëª¨ë¸ í¬ê¸°ê°€ ë§¤ìš° í¬ë¯€ë¡œ(ì•½ 1TB), ë‹¤ìš´ë¡œë“œì— ìƒë‹¹í•œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
# 1. ì €ì¥ì†Œ ê¶Œí•œ ì„¤ì • (/data ê²½ë¡œ)
sudo chown -R ubuntu:ubuntu /data
mkdir -p /data/models/qwen3-coder-480b-a35b-instruct

# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
# --local-dir-use-symlinks False: ìºì‹œê°€ ì•„ë‹Œ ì‹¤ì œ íŒŒì¼ì„ í•´ë‹¹ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤.
huggingface-cli download Qwen/Qwen3-Coder-480B-A35B-Instruct \
--local-dir /data/models/qwen3-coder-480b-a35b-instruct \
--local-dir-use-symlinks False
```

---

## ğŸš€ 3. vLLM ì„œë²„ ì‹¤í–‰ (Server Serving)

Neuron SDKì— ìµœì í™”ëœ ì„¤ì •ì„ ì ìš©í•˜ì—¬ API ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
Qwen3 MoE ëª¨ë¸ì„ ìœ„í•œ `additional-config`ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

> **Note:** ì´ˆê¸° ì‹¤í–‰ ì‹œ ëª¨ë¸ ì»´íŒŒì¼(Compilation) ë° ë¡œë”©ìœ¼ë¡œ ì¸í•´ ì„œë²„ê°€ ì¤€ë¹„(`Uvicorn running...`)ë  ë•Œê¹Œì§€ ìˆ˜ì‹­ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
VLLM_NEURON_FRAMEWORK='neuronx-distributed-inference' python -m vllm.entrypoints.openai.api_server \
  --model="/data/models/qwen3-coder-480b-a35b-instruct" \
  --tensor-parallel-size=64 \
  --max-num-seqs=1 \
  --max-model-len=16384 \
  --additional-config='{"override_neuron_config": {
    "async_mode": false,
    "attn_kernel_enabled": false,
    "batch_size": 1,
    "cc_pipeline_tiling_factor": 1,
    "context_encoding_buckets": [16384],
    "cp_degree": 1,
    "ctx_batch_size": 1,
    "enable_bucketing": true,
    "flash_decoding_enabled": false,
    "fused_qkv": false,
    "is_continuous_batching": true,
    "logical_nc_config": 2,
    "max_context_length": 16384,
    "moe_ep_degree": 1,
    "moe_tp_degree": 64,
    "on_device_sampling_config": {
      "do_sample": true,
      "temperature": 0.6,
      "top_k": 20,
      "top_p": 0.95
    },
    "qkv_cte_nki_kernel_fuse_rope": false,
    "qkv_kernel_enabled": false,
    "qkv_nki_kernel_enabled": false,
    "seq_len": 16384,
    "sequence_parallel_enabled": true,
    "token_generation_buckets": [16384],
    "torch_dtype": "bfloat16",
    "tp_degree": 64
  }}' \
  --no-enable-chunked-prefill \
  --no-enable-prefix-caching \
  --port=8000
```

<img width="1067" height="801" alt="Screenshot 2026-01-28 at 10 39 59â€¯AM" src="https://github.com/user-attachments/assets/4cc59b6f-5351-4222-9bda-12fdc26a72e6" />



---

## ğŸ§ª 4. ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Tool Calling Demo)

ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©´ ì•„ë˜ Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ í†µí•´ Tool Calling ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
vLLM ì„œë²„ ì˜µì…˜ ì œì•½ ì—†ì´ **í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§(System Prompt)** ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ JSON ì¶œë ¥ì„ ìœ ë„í•©ë‹ˆë‹¤.

### `test_inference.py` ì‘ì„±

```python
from openai import OpenAI
import json

# 1. í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(
    base_url='http://localhost:8000/v1', 
    api_key="EMPTY"
)

# 2. ë„êµ¬(Tools) ì •ì˜ë¥¼ 'í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸'ë¡œ ë³€í™˜
# APIì˜ tools íŒŒë¼ë¯¸í„°ë¥¼ ì“°ë©´ ì„œë²„ ì„¤ì • ì¶©ëŒ ê°€ëŠ¥ì„±ì´ ìˆì–´ System Promptë¡œ ì£¼ì…í•©ë‹ˆë‹¤.
system_instruction = """
You are a helpful assistant. You have access to the following tools:

{
  "name": "square_the_number",
  "description": "output the square of the number.",
  "parameters": {
      "input_num": {"type": "number", "description": "The number to square"}
  }
}

If the user asks something that requires a tool, DO NOT generate plain text.
Instead, output a JSON object specifically formatted like this:
{"tool_uses": [{"name": "square_the_number", "arguments": {"input_num": 123}}]}
"""

# 3. ìš”ì²­ ë³´ë‚´ê¸°
print("Thinking (Prompt Engineering Mode)...")

messages = [
    {'role': 'system', 'content': system_instruction},
    {'role': 'user', 'content': 'square the number 1024'}
]

completion = client.chat.completions.create(
    model="/data/models/qwen3-coder-480b-a35b-instruct", # ì„œë²„ ì‹¤í–‰ ì‹œ ì§€ì •í•œ ëª¨ë¸ ê²½ë¡œ
    messages=messages,
    # â˜… ì¤‘ìš”: tools íŒŒë¼ë¯¸í„°ë¥¼ ì‚­ì œí•˜ê³  Promptë¡œ ì œì–´
    max_tokens=1024,
    temperature=0.0, 
)

# 4. ê²°ê³¼ í™•ì¸ ë° íŒŒì‹±
response_text = completion.choices[0].message.content
print(f"â–¼ ëª¨ë¸ ì‘ë‹µ (Raw Text):")
print(response_text)

# JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„ (ëª¨ë¸ì´ ì˜ë„ëŒ€ë¡œ ì‘ë‹µí–ˆëŠ”ì§€ ê²€ì¦)
try:
    if "{" in response_text:
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        tool_call_json = json.loads(response_text[json_start:json_end])
        
        print("\nâ–¼ ì„±ê³µ! ëª¨ë¸ì´ ë„êµ¬ í˜¸ì¶œ JSONì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤:")
        print(json.dumps(tool_call_json, indent=2))
    else:
        print("\nâ–¼ ëª¨ë¸ì´ JSONì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"\nâ–¼ íŒŒì‹± ì—ëŸ¬: {e}")
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
python3 test_inference.py
```
<img width="800" height="906" alt="Screenshot 2026-01-28 at 10 43 13â€¯AM" src="https://github.com/user-attachments/assets/ff42a804-713b-4738-a602-2032c03cf736" />



---

## ğŸ“ Appendix: 
