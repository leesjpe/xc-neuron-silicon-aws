# AWS Trainium2 ê¸°ë°˜ Llama 3 8B Fine-tuning

ì´ ê°€ì´ë“œëŠ” **AWS Trainium2 (Trn2)** ì¸ìŠ¤í„´ìŠ¤ì—ì„œ **LoRA (Low-Rank Adaptation)** ë° **Tensor Parallelism (í…ì„œ ë³‘ë ¬í™”)**ì„ ì‚¬ìš©í•˜ì—¬ [**Llama 3 8B**](https://huggingface.co/meta-llama/Meta-Llama-3-8B) ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë‹¨ê³„ë³„ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ Neuron ê¸°ë°˜ **vLLM**ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” ë°©ë²•ë„ ë‹¤ë£¹ë‹ˆë‹¤.

## Prerequisites

1.  **ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰:**
    * `trn2.48xlarge` ì¸ìŠ¤í„´ìŠ¤ê°€ í™œì„±í™”(`Running`) ìƒíƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
    * ğŸ‘‰ **[ê°€ì´ë“œ: Capacity Block ê¸°ë°˜ XC ì¸ìŠ¤í„´ìŠ¤ ì‹¤í–‰](https://github.com/leesjpe/compute-foundation-on-aws/blob/main/ec2/ec2-dlami-neuron.md)** 
2.  **ê³ ì† ìŠ¤í† ë¦¬ì§€ ì„¤ì •(ì„ íƒ ì‚¬í•­) :**
    * ëª¨ë¸ ë¡œë”© ì†ë„ì™€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë¡œì»¬ NVMe SSD (RAID 0) ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    * ì•„ì§ ì„¤ì •í•˜ì§€ ì•Šìœ¼ì…¨ë‹¤ë©´, ì•„ë˜ ê°€ì´ë“œë¥¼ ë¨¼ì € ì§„í–‰í•´ ì£¼ì„¸ìš”.
    * ğŸ‘‰ **[ê°€ì´ë“œ: ê³ ì† ìŠ¤í† ë¦¬ì§€ ì„¤ì • (NVMe RAID 0)](https://github.com/leesjpe/compute-foundation-on-aws/blob/main/storage/local-nvme-setup.md)**
    * *ì°¸ê³ : ì´ ê³¼ì •ì„ ê±´ë„ˆë›´ë‹¤ë©´, ë£¨íŠ¸ EBS ë³¼ë¥¨ì— ëª¨ë¸ì„ ì €ì¥í•  ì¶©ë¶„í•œ ê³µê°„ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.*

---

## 1. í™˜ê²½ ì„¤ì • (Environment Setup)

### 1.1. Neuron ê°€ìƒí™˜ê²½ í™œì„±í™”

```bash
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_training/bin/activate

```

### 1.2. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ

`aws-neuron/neuronx-distributed` ë¦¬í¬ì§€í† ë¦¬ì—ì„œ í•„ìš”í•œ í•™ìŠµ ë° ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

```bash
# ì‹¤í—˜ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /data/tp_llama3_8b_lora_finetune
cd /data/tp_llama3_8b_lora_finetune

# ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/lightning/data_module.py
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/lightning/module_llama.py
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/lightning/tp_llama_hf_finetune_ptl.py
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/tp_zero1_llama_hf_pretrain/8B_config_llama3/config.json
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/lr.py
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/modeling_llama_nxd.py
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/requirements.txt
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/requirements_ptl.txt
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/training_utils.py
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/examples/training/llama/convert_checkpoints.py
wget https://raw.githubusercontent.com/aws-neuron/neuronx-distributed/main/test/integration/modules/lora/test_llama_lora_finetune.sh
wget https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py

```

### 1.3. íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
python3 -m pip install -r requirements.txt
python3 -m pip install -r requirements_ptl.txt

# ì‰˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x test_llama_lora_finetune.sh
```

### 1.4 NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° punkt ë‹¤ìš´ë¡œë“œ
* NLTK(Natural Language Toolkit) ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í…ìŠ¤íŠ¸ë¥¼ "ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°(Sentence Splitting)" ìœ„í•´ í•„ìš”í•œ í•™ìŠµëœ ëª¨ë¸ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ.
```
pip install nltk
python3 -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab');"

```
ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ë©´ ì•„ë˜ì™€ ê°™ì€ ê²½ë¡œì™€ êµ¬ì¡° í™•ì¸ ê°€ëŠ¥
```bash
/home/ubuntu/nltk_data/  <-- NLTK data ë””ë ‰í„°ë¦¬
â””â”€â”€ tokenizers/          <-- í† í¬ë‚˜ì´ì € ëª¨ë¸ ë””ë ‰í„°ë¦¬
    â””â”€â”€ punkt/           <-- ë‹¤ìš´ë°›ì€ ëª¨ë¸ ë””ë ‰í„°ë¦¬
        â”œâ”€â”€ english.pickle
        â””â”€â”€ ...
```

---

## 2. ëª¨ë¸ ë° ë°ì´í„°ì…‹ ì¤€ë¹„ (Prepare Model & Dataset)

### 2.1. Llama-3-8B ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ (Hugging Face)

Base ëª¨ë¸ì„ ë¡œì»¬ ë””ë ‰í† ë¦¬ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

```bash
hf auth login
# ./models/llama3-8b ê²½ë¡œì— ë‹¤ìš´ë¡œë“œ
hf download meta-llama/Meta-Llama-3-8B --local-dir /data/models/hf-llama3-8b-bf16

```
í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë‹¤ìš´ë¡œë“œ í•˜ì˜€ê¸°ì— ì²´í¬í¬ì¸íŠ¸ ë³€í™˜ì€ ìƒëµ í•˜ì§€ë§Œ ë§Œì•½ Meta í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ í•˜ì˜€ë‹¤ë©´ ë‹¤ìš´ë¡œë“œ ë°›ì€ * [convert_llama_weights_to_hf.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py) ë¡œ ì•„ë˜ì™€ ê°™ì´ HF í˜•ì‹ìœ¼ë¡œ ë³€ê²½ í•„ìš”
```bash
pip install blobfile tiktoken
cd /data/tp_llama3_8b_lora_finetune
python convert_llama_weights_to_hf.py --input_dir /data/models/hf-llama3-8b-bf16/ --model_size 8B --llama_version 3 --output_dir /data/models/conv_hf-llama3-8b-bf16
```

### 2.2. ì²´í¬í¬ì¸íŠ¸ë¥¼ Neuron í¬ë§·(NXD)ìœ¼ë¡œ ë³€í™˜

ë³€í™˜ ê³¼ì • ì¤‘ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´, `AutoModelForCausalLM`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì§ì ‘ ë¡œë“œí•˜ëŠ” ì»¤ìŠ¤í…€ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

1. **`convert_checkpoints.py` ìƒì„±/ìˆ˜ì •:**
[ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸](https://github.com/aws-neuron/neuronx-distributed/blob/main/examples/training/llama/convert_checkpoints.py)ë¥¼ ì•„ë˜ì˜ ìµœì í™”ëœ ì½”ë“œë¡œ ë®ì–´ì”ë‹ˆë‹¤.
ê¸°ì¡´ ë‹¤ìš´ë¡œë“œ ë°›ì€ ìŠ¤í¬ë¦½íŠ¸ëŠ” .bin íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜ í˜„ì¬ HF ëŠ” model.safetensors í¬ë§·ì´ê¸°ì— ì•„ë˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë³€ê²½í•˜ì—¬ Hugging Face(.safetensors)ë¥¼ **ë©”ëª¨ë¦¬(RAM)**ì— ë¡œë“œí•˜ìë§ˆì â†’ ì¦‰ì‹œ Neuron í¬ë§·ìœ¼ë¡œ ë³€í™˜í•´ì„œ ì €ì¥ í•¨.

```python
import argparse
import torch
from transformers import AutoModelForCausalLM
from neuronx_distributed.scripts.checkpoint_converter import CheckpointConverterBase

class CheckpointConverterLlama(CheckpointConverterBase):
    def load_full_state(self, args):
        print(f"Loading model directly from {args.input_dir} using Transformers...")
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            args.input_dir, 
            torch_dtype="auto", 
            low_cpu_mem_usage=True, 
            trust_remote_code=True
        )
        return model.state_dict()

if __name__ == "__main__":
    checkpoint_converter = CheckpointConverterLlama()
    parser = checkpoint_converter.get_arg_parser()
    args, _ = parser.parse_known_args()
    checkpoint_converter.run(args)
```


2. **ë³€í™˜ ì‹¤í–‰:**
Hugging Face í¬ë§·ì„ Neuron Distributed (Megatron ìŠ¤íƒ€ì¼) í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
Hugging Face Transformersê°€ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¨¼ì € ì„¤ì¹˜ í•©ë‹ˆë‹¤.

```bash
pip install accelerate

python3 convert_checkpoints.py \
--hw_backend trn2 \
--tp_size 32 \
--qkv_linear 1 \
--kv_size_multiplier 4 \
--convert_from_full_state \
--config config.json \
--input_dir /data/models/hf-llama3-8b-bf16 \
--output_dir /data/models/llama3_8b_bf16-tp32/pretrained_weight/

```

* `--tp_size 32`: íƒ€ê²Ÿ Tensor Parallelism í¬ê¸° (Trn2 ë…¸ë“œ ì‚¬ì–‘ì— ë§ì¶¤).
* `--hw_backend trn2`: íƒ€ê²Ÿ í•˜ë“œì›¨ì–´ ì„¤ì •.
* `--qkv_linear`: GQA(Grouped-Query Attentioin) ëª¨ë¸ì€ 1, Non GQA ëª¨ë¸ì€ 0

[ì‹¤í–‰ê²°ê³¼]
<img width="1107" height="630" alt="Screenshot 2026-01-16 at 3 56 55â€¯PM" src="https://github.com/user-attachments/assets/505bbe66-4282-4cd5-9b34-77bced621c41" />


---

## 3. LoRA íŒŒì¸íŠœë‹ (Fine-tuning with LoRA)

### 3.1. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •

`test_llama_lora_finetune.sh` íŒŒì¼ì„ ì—´ì–´ ê²½ë¡œ ë° í•™ìŠµ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.

```bash
# ê²½ë¡œ ì„¤ì •
PRETRAINED_PATH=/data/models/llama3_8b_bf16-tp32
BASE_MODEL=/data/models/hf-llama3-8b-bf16
HF_TOKEN='your_token_here'

# í•™ìŠµ íŒŒë¼ë¯¸í„° (Full Fine-tuning)
# ì „ì²´ Epochë¥¼ ëŒë¦¬ê¸° ìœ„í•´ step ì œí•œì„ í•´ì œ(-1)í•©ë‹ˆë‹¤.
TOTAL_STEPS=-1 
TOTAL_EPOCHS=3
```
> **ì£¼ì˜:** ìŠ¤í¬ë¦½íŠ¸ ë‚´ì— `max_train_samples` ì˜µì…˜ì´ ìˆë‹¤ë©´ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œí•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ì„ í•™ìŠµí•˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.

### 3.2. í•™ìŠµ ë°ì´í„°
ì´ ì˜ˆì‹œì—ì„œëŠ” InstructGPT ë…¼ë¬¸ì—ì„œ ì„¤ëª…ëœ ë²”ì£¼(ë¸Œë ˆì¸ìŠ¤í† ë°, ë¶„ë¥˜, íì‡„í˜• ì§ˆì˜ì‘ë‹µ, ìƒì„±, ì •ë³´ ì¶”ì¶œ, ê°œë°©í˜• ì§ˆì˜ì‘ë‹µ ë° ìš”ì•½ í¬í•¨)ì— ëŒ€í•œ ì§€ì‹œ ë”°ë¥´ê¸° ê¸°ë¡ìœ¼ë¡œ êµ¬ì„±ëœ ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„°ì…‹ì¸ Dollyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ë°ì´í„°ì…‹ì„ ì„¤ì •í•˜ë ¤ë©´ test_llama_lora_finetune.sh íŒŒì¼ì—ì„œ ë‹¤ìŒ í”Œë˜ê·¸ë“¤ì„ êµ¬ì„±í•©ë‹ˆë‹¤. 

```bash
--data_dir "databricks/databricks-dolly-15k" \
--task "open_qa" \
```

### 3.3. í•™ìŠµ ì‹œì‘

```bash
./test_llama_lora_finetune.sh
```

**ì™„ë£Œ í™•ì¸:**

* ë¡œê·¸ ë©”ì‹œì§€: `Training finished!`
* ë¡œê·¸ ë©”ì‹œì§€: `synced saving of checkpoint lora completed`

Trn2.32xlarge ì˜ ê²½ìš° ìˆ˜ë¶„ë‚´ë¡œ ë§ˆë¬´ë¦¬ ë©ë‹ˆë‹¤. 

### 3.4. ê²°ê³¼ í™•ì¸

LoRA ì–´ëŒ‘í„°ëŠ” ì¶œë ¥ ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤. NXDëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶„í• ëœ(sharded) `.pt` íŒŒì¼ë“¤ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

```bash
lora_adapter/
â”œâ”€â”€ adapter_config.json
â””â”€â”€ lora/model/
    â”œâ”€â”€ dp_rank_00_tp_rank_00_pp_rank_00.pt
    ... (TP=32ì¸ ê²½ìš° 32ê°œ íŒŒì¼)
```

---

## 4. vLLMì„ ì´ìš©í•œ ì¶”ë¡  (Inference with vLLM)

### 4.1. vLLMìš© LoRA ì–´ëŒ‘í„° ì¤€ë¹„ (ì¤‘ìš”)

Neuron ê¸°ë°˜ vLLMì€ ë¶„í• ëœ `.pt` íŒŒì¼ë“¤ì´ `lora/model`ê³¼ ê°™ì€ í•˜ìœ„ í´ë”ê°€ ì•„ë‹Œ, ì–´ëŒ‘í„° ë””ë ‰í† ë¦¬ ìµœìƒìœ„ì— ìœ„ì¹˜ í•´ì•¼ í•©ë‹ˆë‹¤.

1. **ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì´ë™:**
```bash
cd /home/ubuntu/tp_llama3_8b_lora_finetune/lora_adapter/lora/model
cp *.pt ../../
```

*ì´ì œ `adapter_config.json`ê³¼ `*.pt` íŒŒì¼ë“¤ì´ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.*

2. **`adapter_config.json` ìˆ˜ì •:**

Neuron vLLMì€ Q, K, V ë ˆì´ì–´ë¥¼ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤. ë”°ë¼ì„œ `target_modules` ì´ë¦„ì„ ì´ì— ë§ì¶° ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.

**ë³€ê²½ ì „:**
```json
"target_modules": ["qkv_proj"],
```

**ë³€ê²½ í›„:**
```json
"target_modules": ["q_proj", "k_proj", "v_proj"],
```

3. ë§¤í•‘ íŒŒì¼ ìƒì„± (lora_serving_config.json)
ì•„ë˜ ë‚´ìš©ì„ ë‹´ì€ json íŒŒì¼ì„ ìƒì„±

```bash
vi /data/tp_llama3_8b_lora_finetune/lora_serving_config.json

{
  "lora-ckpt-paths": {
    "llama3_adapter": "/data/tp_llama3_8b_lora_finetune/lora_adapter"
  },
  "lora-ckpt-paths-cpu": {}
}
```
* ì„¤ëª…: "llama3_adapter"ëŠ” ì‚¬ìš©ìê°€ ì§€ì • ì´ë¦„, ë’¤ì—ëŠ” .pt íŒŒì¼ë“¤ì´ ë“¤ì–´ìˆëŠ” ì‹¤ì œ í´ë” ê²½ë¡œë¥¼ ì§€ì •.


### 4.2.1 vllm í™˜ê²½ ì‚¬ìš©
3ë²ˆê¹Œì§€ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•œ ê°€ìƒí™˜ê²½ì´ Enable ë˜ì–´ìˆë‹¤ë©´ deactivate í›„ ì‚¬ì „ êµ¬ì„±ëœ vllm í™˜ê²½ í™œì„±í™”
```bash
deactivate
source /opt/aws_neuronx_venv_pytorch_inference_vllm/bin/activate

```

### 4.2.2 vLLM ë„ì»¤ ì»¨í…Œì´ë„ˆ ì‹¤í–‰

* vllm í™˜ê²½ì„ ìœ„í•œ Docker container ì‹¤í–‰

```bash
docker pull public.ecr.aws/neuron/pytorch-inference-vllm-neuronx:<image_tag>
# neuron 2.27 vllm 0.11.0 ê¸°ì¤€ tag
docker pull public.ecr.aws/neuron/pytorch-inference-vllm-neuronx:0.11.0-neuronx-py312-sdk2.27.0-ubuntu24.04

docker run \
-d -it \
-v /home/ubuntu/:/home/ubuntu/ \
-v /data:/data \
--privileged \
--cap-add SYS_ADMIN \
--cap-add IPC_LOCK \
-p 8000:8000 \
--name <server name> \
<Image ID>
```

### 4.3. ì¶”ë¡  TEST

* **LoRA ì–´ëŒ‘í„°ë¥¼ ì ìš©í•˜ì—¬ ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. (`test_lora_inference.py`)**

```python
import os
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# ==========================================
# ì‚¬ìš©ì í™˜ê²½ ê²½ë¡œ ë°˜ì˜
# ==========================================
MODEL_PATH = "/data/models/llama3-8b"
LORA_CKPT_JSON = "/data/tp_llama3_8b_lora_finetune/lora_serving_config.json"
COMPILED_MODEL_PATH = "/data/cache/llama3-8b-lora-finetuned-neuron_cache"

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["NEURON_COMPILED_ARTIFACTS"] = COMPILED_MODEL_PATH
os.environ["VLLM_USE_V1"] = "1"

# Sample prompts.
prompts = [
    "The president of the United States is"
]

# Create a sampling params object.
sampling_params = SamplingParams(top_k=1, max_tokens=4096)

# override_neuron_config êµ¬ì¡°
override_neuron_config = {
    "skip_warmup": True,
    "lora_ckpt_json": LORA_CKPT_JSON,
}

# Create an LLM with multi-LoRA serving.
# additional_config í¬í•¨í•œ ì´ˆê¸°í™” ì½”ë“œ
llm = LLM(
    model=MODEL_PATH,
    max_num_seqs=2,
    max_model_len=4096,           # 64ëŠ” ë„ˆë¬´ ì§§ì•„ 4096ìœ¼ë¡œ ìˆ˜ì •í•¨ (ì•ˆì •ì„± ìœ„í•¨)
    tensor_parallel_size=32,
    additional_config={
        "override_neuron_config": override_neuron_config
    },
    enable_lora=True,
    max_loras=2,
    max_cpu_loras=4,
    enable_prefix_caching=False,
    enable_chunked_prefill=False,
)

"""
Only the lora_name needs to be specified.
The lora_id and lora_path are supplied at the LLM class/server initialization, after which the paths are
handled by NxD Inference.
"""

# lora_id_1 is in HBM (Defined in JSON)
lora_req_1 = LoRARequest("lora_id_1", 1, lora_path="/home/ubuntu/tp_llama3_8b_lora_finetune/lora_adapters/llama3_8b_lora") # Path is empty as per JSON usage
# lora_id_3 is in host memory (Defined in JSON)
#lora_req_2 = LoRARequest("lora_id_3", 2, lora_path="") # Path is empty as per JSON usage

#outputs = llm.generate(prompts, sampling_params, lora_request=[lora_req_1, lora_req_2])
outputs = llm.generate(prompts, sampling_params, lora_request=[lora_req_1])

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

```


### 4.4. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì²´í¬ë¦¬ìŠ¤íŠ¸

* **ë‹µë³€ì´ ì¤‘ê°„ì— ì˜ë¦¬ë‚˜ìš”?** `SamplingParams`ì˜ `max_tokens` ê°’ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.
* **ê²°ê³¼ê°€ Base ëª¨ë¸ê³¼ ë˜‘ê°™ë‚˜ìš”?**
* ì—¬ëŸ¬ ë²ˆ í•™ìŠµí–ˆë‹¤ë©´ `checkpoint-xxx` í´ë”ê°€ ìƒˆë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ê²½ë¡œë¥¼ ìµœì‹ ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.
* `adapter_config.json`ì˜ `target_modules`ê°€ `["q_proj", "k_proj", "v_proj"]`ë¡œ ì •í™•íˆ ìˆ˜ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
---

## Additional: Offline inference
